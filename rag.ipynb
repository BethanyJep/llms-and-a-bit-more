{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1001859",
   "metadata": {},
   "source": [
    "# Simple RAG Implementation with Constitution of Kenya\n",
    "\n",
    "This notebook implements a simple Retrieval-Augmented Generation (RAG) system using the Constitution of Kenya 2010 PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d972e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in ./.venv/lib/python3.13/site-packages (1.26.3)\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.13/site-packages (5.0.0)\n",
      "Requirement already satisfied: faiss-cpu in ./.venv/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.13/site-packages (1.93.3)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.13/site-packages (0.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.13/site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.13/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.13/site-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.13/site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.13/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.venv/lib/python3.13/site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.13/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pymupdf sentence-transformers faiss-cpu openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cceec536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pikachu/Downloads/llms-and-a-bit-more/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ba0e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded: 172217 characters\n",
      "First 300 characters: KENYA ARTIFICIAL INTELLIGENCE STRATEGY 2025-2030 March 2025 \n",
      "\n",
      "\n",
      "\n",
      "KENYA ARTIFICIAL INTELLIGENCE STRATEGY 2025-2030 March 2025 \n",
      "\n",
      "4 / Kenya AI Strategy List of Abbreviations.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.6 EXECUTIVE SUMMAR.......................7 FOREWOR.........................................9...\n"
     ]
    }
   ],
   "source": [
    "# Load PDF and extract text\n",
    "def load_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page in doc:\n",
    "        page_text = page.get_text()\n",
    "        # Clean up text\n",
    "        page_text = re.sub(r'\\s+', ' ', page_text)\n",
    "        text += page_text + \"\\n\\n\"\n",
    "    \n",
    "    doc.close()\n",
    "    return text.strip()\n",
    "\n",
    "# Load the Constitution PDF\n",
    "pdf_path = \"/Users/pikachu/Downloads/llms-and-a-bit-more/Signed National-AI-Strategy_Final 26 Mar25.pdf\"\n",
    "strategy_text = load_pdf(pdf_path)\n",
    "\n",
    "print(f\"Document loaded: {len(strategy_text)} characters\")\n",
    "print(f\"First 300 characters: {strategy_text[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ecbb3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 31 chunks\n",
      "Sample chunk: KENYA ARTIFICIAL INTELLIGENCE STRATEGY 2025-2030 March 2025 KENYA ARTIFICIAL INTELLIGENCE STRATEGY 2025-2030 March 2025 4 / Kenya AI Strategy List of Abbreviations.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.ÔøΩ.6 ...\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "def split_text(text, chunk_size=1000, overlap=200):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        if i + chunk_size >= len(words):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create chunks\n",
    "chunks = split_text(strategy_text)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"Sample chunk: {chunks[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f817db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (31, 384)\n",
      "Embeddings created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "print(\"Loading embedding model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(\"Embeddings created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b0b63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up vector database...\n",
      "Vector database created with 31 vectors\n",
      "Ready for similarity search!\n"
     ]
    }
   ],
   "source": [
    "# Create vector database\n",
    "print(\"Setting up vector database...\")\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "index.add(normalized_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"Vector database created with {index.ntotal} vectors\")\n",
    "print(\"Ready for similarity search!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "944379d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG query function ready!\n",
      "Use: query_strategy on AI to search the Stratregy document\n"
     ]
    }
   ],
   "source": [
    "# Simple RAG query function\n",
    "def query_strategy(question, k=3):\n",
    "    \"\"\"Query the strategy and return relevant chunks\"\"\"\n",
    "    \n",
    "    # Create embedding for the question\n",
    "    question_embedding = model.encode([question])\n",
    "    \n",
    "    # Normalize question embedding\n",
    "    question_embedding = question_embedding / np.linalg.norm(question_embedding)\n",
    "    \n",
    "    # Search for similar chunks\n",
    "    scores, indices = index.search(question_embedding.astype('float32'), k)\n",
    "    \n",
    "    # Collect results\n",
    "    results = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        results.append({\n",
    "            'chunk': chunks[idx],\n",
    "            'score': float(score),\n",
    "            'rank': i + 1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"RAG query function ready!\")\n",
    "print(\"Use: query_strategy on AI to search the Stratregy document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f18c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask a sample question\n",
    "sample_questions = [\n",
    "    \"What are the key objectives of the National AI Strategy?\",\n",
    "    \"How does the strategy address ethical considerations in AI?\",\n",
    "    \"What are the main challenges identified in the strategy?\",\n",
    "    \"What is the role of public-private partnerships in the strategy?\",\n",
    "]\n",
    "\n",
    "def query_strategy(question: str, top_k: int = 5):\n",
    "    # Generate embedding for the question\n",
    "    question_embedding = model.encode([question])[0]\n",
    "    question_embedding = question_embedding / np.linalg.norm(question_embedding)  # Normalize\n",
    "    \n",
    "    # Search in the vector database\n",
    "    distances, indices = index.search(np.array([question_embedding], dtype='float32'), top_k)\n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        results.append({\n",
    "            'chunk': chunks[idx],\n",
    "            'distance': distances[0][i]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = query_strategy(\"What are the key objectives of the National AI Strategy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb1f1471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION: What are the key objectives of the National AI Strategy?\n",
      "============================================================\n",
      "\n",
      "Rank 1 (Score: 0.650)\n",
      "Text: As a leading hub for technology and innovation in Africa, Kenya is well positioned to provide leadership and set the pace for how AI can be applied to address our unique challenges and drive sustainable development. By integrating AI into critical sectors‚Äîincluding agriculture, healthcare, finance, ...\n",
      "----------------------------------------\n",
      "\n",
      "Rank 2 (Score: 0.599)\n",
      "Text: potential of AI to improve the economic growth, productivity, and quality of life for the Mauritian state. The focal areas of the plan include matching existing and new AI solutions to specific sectors and regions, establishing a ‚ÄúMauritian unique selling point‚Äù of AI, building an appropriate ecosys...\n",
      "----------------------------------------\n",
      "\n",
      "Rank 3 (Score: 0.582)\n",
      "Text: / Kenya AI Strategy This strategy aims to be comprehensive, addressing multiple facets of AI development, adoption, and governance. The strategy will create a holistic framework that creates an enabling environment for AI innovation and adoption but also ensures that this technological revolution be...\n",
      "----------------------------------------\n",
      "\n",
      "Rank 4 (Score: 0.578)\n",
      "Text: adhering to ethical principles and inclusivity. Developed through extensive consultations with government agencies, private-sector stakeholders, academia, civil society, international partners, and local communities, the strategy reflects a participatory approach that aligns with Kenya‚Äôs national va...\n",
      "----------------------------------------\n",
      "\n",
      "Rank 5 (Score: 0.566)\n",
      "Text: number of young Kenyans pursuing advanced studies in AI. Despite these exciting emerging opportunities and the potential for AI to revolutionise the Kenyan economy, there are a number of growing concerns, particularly regarding the governance of the technology and the ability of Kenyan citizens to f...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fix the query_strategy function to match what display_results expects\n",
    "def query_strategy(question: str, k: int = 5):\n",
    "    \"\"\"Query the strategy and return relevant chunks\"\"\"\n",
    "    \n",
    "    # Create embedding for the question\n",
    "    question_embedding = model.encode([question])\n",
    "    \n",
    "    # Normalize question embedding\n",
    "    question_embedding = question_embedding / np.linalg.norm(question_embedding)\n",
    "    \n",
    "    # Search for similar chunks\n",
    "    scores, indices = index.search(question_embedding.astype('float32'), k)\n",
    "    \n",
    "    # Collect results\n",
    "    results = []\n",
    "    for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "        results.append({\n",
    "            'chunk': chunks[idx],\n",
    "            'score': float(score),  # This matches what display_results expects\n",
    "            'rank': i + 1          # This matches what display_results expects\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to display results nicely\n",
    "def display_results(question, results):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"\\nRank {result['rank']} (Score: {result['score']:.3f})\")\n",
    "        print(f\"Text: {result['chunk'][:300]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Test with first question\n",
    "question = sample_questions[0]\n",
    "results = query_strategy(question)\n",
    "display_results(question, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b13f9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-evaluation in ./.venv/lib/python3.13/site-packages (1.10.0)\n",
      "Requirement already satisfied: azure-identity in ./.venv/lib/python3.13/site-packages (1.23.1)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: pyjwt>=2.8.0 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (2.10.1)\n",
      "Requirement already satisfied: azure-core>=1.30.2 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (1.35.0)\n",
      "Requirement already satisfied: nltk>=3.9.1 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (3.9.1)\n",
      "Requirement already satisfied: azure-storage-blob>=12.10.0 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (12.26.0)\n",
      "Requirement already satisfied: httpx>=0.25.1 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (0.28.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.2 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (2.3.1)\n",
      "Requirement already satisfied: openai>=1.78.0 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (1.93.3)\n",
      "Requirement already satisfied: ruamel.yaml<1.0.0,>=0.17.10 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (0.18.14)\n",
      "Requirement already satisfied: msrest>=0.6.21 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (0.7.1)\n",
      "Requirement already satisfied: Jinja2>=3.1.6 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (3.1.6)\n",
      "Requirement already satisfied: aiohttp>=3.0 in ./.venv/lib/python3.13/site-packages (from azure-ai-evaluation) (3.12.15)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas<3.0.0,>=2.1.2->azure-ai-evaluation) (2025.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in ./.venv/lib/python3.13/site-packages (from ruamel.yaml<1.0.0,>=0.17.10->azure-ai-evaluation) (0.2.12)\n",
      "Requirement already satisfied: cryptography>=2.5 in ./.venv/lib/python3.13/site-packages (from azure-identity) (45.0.5)\n",
      "Requirement already satisfied: msal>=1.30.0 in ./.venv/lib/python3.13/site-packages (from azure-identity) (1.33.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in ./.venv/lib/python3.13/site-packages (from azure-identity) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.13/site-packages (from azure-identity) (4.14.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp>=3.0->azure-ai-evaluation) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.13/site-packages (from aiohttp>=3.0->azure-ai-evaluation) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp>=3.0->azure-ai-evaluation) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp>=3.0->azure-ai-evaluation) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp>=3.0->azure-ai-evaluation) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp>=3.0->azure-ai-evaluation) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp>=3.0->azure-ai-evaluation) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.0->azure-ai-evaluation) (3.10)\n",
      "Requirement already satisfied: requests>=2.21.0 in ./.venv/lib/python3.13/site-packages (from azure-core>=1.30.2->azure-ai-evaluation) (2.32.4)\n",
      "Requirement already satisfied: six>=1.11.0 in ./.venv/lib/python3.13/site-packages (from azure-core>=1.30.2->azure-ai-evaluation) (1.17.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in ./.venv/lib/python3.13/site-packages (from azure-storage-blob>=12.10.0->azure-ai-evaluation) (0.7.2)\n",
      "Requirement already satisfied: cffi>=1.14 in ./.venv/lib/python3.13/site-packages (from cryptography>=2.5->azure-identity) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.13/site-packages (from cffi>=1.14->cryptography>=2.5->azure-identity) (2.22)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.1->azure-ai-evaluation) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.1->azure-ai-evaluation) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.1->azure-ai-evaluation) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.1->azure-ai-evaluation) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from Jinja2>=3.1.6->azure-ai-evaluation) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.21.0->azure-core>=1.30.2->azure-ai-evaluation) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.21.0->azure-core>=1.30.2->azure-ai-evaluation) (2.5.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in ./.venv/lib/python3.13/site-packages (from msrest>=0.6.21->azure-ai-evaluation) (2.0.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.13/site-packages (from nltk>=3.9.1->azure-ai-evaluation) (8.2.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.13/site-packages (from nltk>=3.9.1->azure-ai-evaluation) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.13/site-packages (from nltk>=3.9.1->azure-ai-evaluation) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from nltk>=3.9.1->azure-ai-evaluation) (4.67.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from openai>=1.78.0->azure-ai-evaluation) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.13/site-packages (from openai>=1.78.0->azure-ai-evaluation) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.13/site-packages (from openai>=1.78.0->azure-ai-evaluation) (2.11.7)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from openai>=1.78.0->azure-ai-evaluation) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai>=1.78.0->azure-ai-evaluation) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai>=1.78.0->azure-ai-evaluation) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai>=1.78.0->azure-ai-evaluation) (0.4.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.venv/lib/python3.13/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-evaluation) (3.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Azure AI Evaluation SDK for Azure AI Foundry evaluators\n",
    "!pip install azure-ai-evaluation azure-identity python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64085be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI configuration loaded successfully!\n",
      "Deployment: gpt-4.1-mini\n",
      "‚úÖ Azure AI Foundry evaluators initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    RetrievalEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    evaluate\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI configuration for evaluators\n",
    "# You'll need to set these environment variables or replace with your actual values\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  # e.g., \"https://your-resource.openai.azure.com/\"\n",
    "    \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),  # e.g., \"gpt-4\"\n",
    "    \"api_version\": \"2025-01-01-preview\"\n",
    "}\n",
    "\n",
    "# Check if configuration is available\n",
    "config_available = all([\n",
    "    model_config[\"azure_endpoint\"],\n",
    "    model_config[\"api_key\"], \n",
    "    model_config[\"azure_deployment\"]\n",
    "])\n",
    "\n",
    "if config_available:\n",
    "    print(\"‚úÖ Azure OpenAI configuration loaded successfully!\")\n",
    "    print(f\"Deployment: {model_config['azure_deployment']}\")\n",
    "    \n",
    "    # Initialize evaluators\n",
    "    try:\n",
    "        groundedness_evaluator = GroundednessEvaluator(model_config=model_config)\n",
    "        relevance_evaluator = RelevanceEvaluator(model_config=model_config)\n",
    "        coherence_evaluator = CoherenceEvaluator(model_config=model_config)\n",
    "        fluency_evaluator = FluencyEvaluator(model_config=model_config)\n",
    "        print(\"‚úÖ Azure AI Foundry evaluators initialized successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error initializing evaluators: {e}\")\n",
    "        print(\"Continuing without AI-assisted evaluators...\")\n",
    "        groundedness_evaluator = None\n",
    "        relevance_evaluator = None\n",
    "        coherence_evaluator = None\n",
    "        fluency_evaluator = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Azure OpenAI configuration not found!\")\n",
    "    print(\"Please set the following environment variables:\")\n",
    "    print(\"Or modify the model_config dictionary above with your values.\")\n",
    "    print(\"Continuing without AI-assisted evaluators...\")\n",
    "    groundedness_evaluator = None\n",
    "    relevance_evaluator = None\n",
    "    coherence_evaluator = None\n",
    "    fluency_evaluator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0249333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced RAG system with Azure AI Foundry evaluators initialized!\n",
      "Available evaluators: ['groundedness', 'relevance', 'coherence', 'fluency']\n"
     ]
    }
   ],
   "source": [
    "# Enhanced RAG query function with evaluation support and observability\n",
    "class EnhancedRAGSystem:\n",
    "    def __init__(self, chunks, index, model, evaluators=None):\n",
    "        self.chunks = chunks\n",
    "        self.index = index\n",
    "        self.model = model\n",
    "        self.evaluators = evaluators or {}\n",
    "        self.query_history = []\n",
    "        \n",
    "    def query_with_context(self, question: str, k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Enhanced query function that returns data in Azure AI Foundry evaluation format\n",
    "        \"\"\"\n",
    "        # Create embedding for the question\n",
    "        question_embedding = self.model.encode([question])\n",
    "        \n",
    "        # Normalize question embedding\n",
    "        question_embedding = question_embedding / np.linalg.norm(question_embedding)\n",
    "        \n",
    "        # Search for similar chunks\n",
    "        scores, indices = self.index.search(question_embedding.astype('float32'), k)\n",
    "        \n",
    "        # Collect retrieved chunks\n",
    "        retrieved_chunks = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            retrieved_chunks.append({\n",
    "                'chunk': self.chunks[idx],\n",
    "                'score': float(score),\n",
    "                'rank': i + 1,\n",
    "                'chunk_id': int(idx)\n",
    "            })\n",
    "        \n",
    "        # Create context from top chunks (concatenate all retrieved text)\n",
    "        context = \"\\n\\n\".join([chunk['chunk'] for chunk in retrieved_chunks])\n",
    "        \n",
    "        # For now, we'll create a simple response (you can integrate with a generation model later)\n",
    "        # This is a placeholder - in a full RAG system you'd use an LLM to generate the response\n",
    "        response = f\"Based on the retrieved information, here are the key points about '{question}': {context[:500]}...\"\n",
    "        \n",
    "        # Structure data for Azure AI Foundry evaluation\n",
    "        result = {\n",
    "            'query': question,\n",
    "            'response': response,\n",
    "            'context': context,\n",
    "            'retrieved_chunks': retrieved_chunks,\n",
    "            'metadata': {\n",
    "                'num_chunks_retrieved': k,\n",
    "                'top_score': float(scores[0][0]) if len(scores[0]) > 0 else 0.0,\n",
    "                'timestamp': str(np.datetime64('now'))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Store for observability\n",
    "        self.query_history.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def evaluate_query_result(self, result: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate a query result using Azure AI Foundry evaluators\n",
    "        \"\"\"\n",
    "        evaluation_scores = {}\n",
    "        \n",
    "        if not self.evaluators:\n",
    "            print(\"‚ö†Ô∏è No evaluators available for evaluation\")\n",
    "            return evaluation_scores\n",
    "            \n",
    "        try:\n",
    "            # Groundedness evaluation\n",
    "            if 'groundedness' in self.evaluators and self.evaluators['groundedness']:\n",
    "                try:\n",
    "                    groundedness_result = self.evaluators['groundedness'](\n",
    "                        query=result['query'],\n",
    "                        response=result['response'],\n",
    "                        context=result['context']\n",
    "                    )\n",
    "                    evaluation_scores['groundedness'] = groundedness_result.get('groundedness', 0)\n",
    "                    evaluation_scores['groundedness_reason'] = groundedness_result.get('gpt_groundedness_reason', 'N/A')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in groundedness evaluation: {e}\")\n",
    "            \n",
    "            # Relevance evaluation\n",
    "            if 'relevance' in self.evaluators and self.evaluators['relevance']:\n",
    "                try:\n",
    "                    relevance_result = self.evaluators['relevance'](\n",
    "                        query=result['query'],\n",
    "                        response=result['response']\n",
    "                    )\n",
    "                    evaluation_scores['relevance'] = relevance_result.get('relevance', 0)\n",
    "                    evaluation_scores['relevance_reason'] = relevance_result.get('gpt_relevance_reason', 'N/A')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in relevance evaluation: {e}\")\n",
    "            \n",
    "            # Coherence evaluation\n",
    "            if 'coherence' in self.evaluators and self.evaluators['coherence']:\n",
    "                try:\n",
    "                    coherence_result = self.evaluators['coherence'](\n",
    "                        query=result['query'],\n",
    "                        response=result['response']\n",
    "                    )\n",
    "                    evaluation_scores['coherence'] = coherence_result.get('coherence', 0)\n",
    "                    evaluation_scores['coherence_reason'] = coherence_result.get('gpt_coherence_reason', 'N/A')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in coherence evaluation: {e}\")\n",
    "            \n",
    "            # Fluency evaluation\n",
    "            if 'fluency' in self.evaluators and self.evaluators['fluency']:\n",
    "                try:\n",
    "                    fluency_result = self.evaluators['fluency'](\n",
    "                        query=result['query'],\n",
    "                        response=result['response']\n",
    "                    )\n",
    "                    evaluation_scores['fluency'] = fluency_result.get('fluency', 0)\n",
    "                    evaluation_scores['fluency_reason'] = fluency_result.get('gpt_fluency_reason', 'N/A')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in fluency evaluation: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation: {e}\")\n",
    "        \n",
    "        return evaluation_scores\n",
    "    \n",
    "    def save_evaluation_dataset(self, filename: str = \"rag_evaluation_dataset.jsonl\"):\n",
    "        \"\"\"\n",
    "        Save query history as JSONL for Azure AI Foundry evaluation\n",
    "        \"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            for entry in self.query_history:\n",
    "                # Format for Azure AI Foundry evaluation\n",
    "                eval_entry = {\n",
    "                    \"query\": entry['query'],\n",
    "                    \"response\": entry['response'],\n",
    "                    \"context\": entry['context']\n",
    "                }\n",
    "                f.write(json.dumps(eval_entry) + '\\n')\n",
    "        print(f\"‚úÖ Saved {len(self.query_history)} entries to {filename}\")\n",
    "    \n",
    "    def get_observability_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get observability summary of the RAG system\n",
    "        \"\"\"\n",
    "        if not self.query_history:\n",
    "            return {\"message\": \"No queries processed yet\"}\n",
    "        \n",
    "        total_queries = len(self.query_history)\n",
    "        avg_top_score = np.mean([q['metadata']['top_score'] for q in self.query_history])\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": total_queries,\n",
    "            \"average_retrieval_score\": avg_top_score,\n",
    "            \"latest_query\": self.query_history[-1]['query'],\n",
    "            \"chunks_per_query\": self.query_history[-1]['metadata']['num_chunks_retrieved']\n",
    "        }\n",
    "\n",
    "# Initialize the enhanced RAG system\n",
    "evaluators_dict = {}\n",
    "if config_available:\n",
    "    if groundedness_evaluator:\n",
    "        evaluators_dict['groundedness'] = groundedness_evaluator\n",
    "    if relevance_evaluator:\n",
    "        evaluators_dict['relevance'] = relevance_evaluator\n",
    "    if coherence_evaluator:\n",
    "        evaluators_dict['coherence'] = coherence_evaluator\n",
    "    if fluency_evaluator:\n",
    "        evaluators_dict['fluency'] = fluency_evaluator\n",
    "\n",
    "enhanced_rag = EnhancedRAGSystem(\n",
    "    chunks=chunks,\n",
    "    index=index,\n",
    "    model=model,\n",
    "    evaluators=evaluators_dict\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Enhanced RAG system with Azure AI Foundry evaluators initialized!\")\n",
    "print(f\"Available evaluators: {list(evaluators_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a492ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing Enhanced RAG System with Azure AI Foundry Evaluators\n",
      "\n",
      "================================================================================\n",
      "üîç TESTING QUERY: What are the key objectives of the National AI Strategy?\n",
      "================================================================================\n",
      "\n",
      "üìù RESPONSE:\n",
      "Based on the retrieved information, here are the key points about 'What are the key objectives of the National AI Strategy?': As a leading hub for technology and innovation in Africa, Kenya is well positioned to provide leadership and set the pace for how AI can be applied to address our unique chal...\n",
      "\n",
      "üìö RETRIEVED CONTEXT:\n",
      "\n",
      "Chunk 1 (Score: 0.650):\n",
      "As a leading hub for technology and innovation in Africa, Kenya is well positioned to provide leadership and set the pace for how AI can be applied to address our unique challenges and drive sustainab...\n",
      "\n",
      "Chunk 2 (Score: 0.599):\n",
      "potential of AI to improve the economic growth, productivity, and quality of life for the Mauritian state. The focal areas of the plan include matching existing and new AI solutions to specific sector...\n",
      "\n",
      "üìä EVALUATION RESULTS:\n",
      "  ‚Ä¢ GROUNDEDNESS: 4.0/5\n",
      "  ‚Ä¢ RELEVANCE: 3.0/5\n",
      "  ‚Ä¢ COHERENCE: 4.0/5\n",
      "  ‚Ä¢ FLUENCY: 4.0/5\n",
      "\n",
      "================================================================================\n",
      "üîç TESTING QUERY: How does the strategy address ethical considerations in AI?\n",
      "================================================================================\n",
      "\n",
      "üìù RESPONSE:\n",
      "Based on the retrieved information, here are the key points about 'How does the strategy address ethical considerations in AI?': As a leading hub for technology and innovation in Africa, Kenya is well positioned to provide leadership and set the pace for how AI can be applied to address our unique c...\n",
      "\n",
      "üìö RETRIEVED CONTEXT:\n",
      "\n",
      "Chunk 1 (Score: 0.557):\n",
      "As a leading hub for technology and innovation in Africa, Kenya is well positioned to provide leadership and set the pace for how AI can be applied to address our unique challenges and drive sustainab...\n",
      "\n",
      "Chunk 2 (Score: 0.547):\n",
      "seen through governance practices of the European Union, the United Kingdom, and the United States, reflecting a growing recognition of the need for ethical AI usage characterised by the different app...\n",
      "\n",
      "üìä EVALUATION RESULTS:\n",
      "  ‚Ä¢ GROUNDEDNESS: 2.0/5\n",
      "  ‚Ä¢ RELEVANCE: 3.0/5\n",
      "  ‚Ä¢ COHERENCE: 3.0/5\n",
      "  ‚Ä¢ FLUENCY: 4.0/5\n"
     ]
    }
   ],
   "source": [
    "# Test the enhanced RAG system with evaluations\n",
    "def test_query_with_evaluation(question: str):\n",
    "    \"\"\"Test a query and evaluate the results\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîç TESTING QUERY: {question}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Get query result\n",
    "    result = enhanced_rag.query_with_context(question, k=3)\n",
    "    \n",
    "    # Display query results\n",
    "    print(f\"\\nüìù RESPONSE:\")\n",
    "    print(f\"{result['response'][:300]}...\")\n",
    "    \n",
    "    print(f\"\\nüìö RETRIEVED CONTEXT:\")\n",
    "    for i, chunk in enumerate(result['retrieved_chunks'][:2]):  # Show top 2 chunks\n",
    "        print(f\"\\nChunk {i+1} (Score: {chunk['score']:.3f}):\")\n",
    "        print(f\"{chunk['chunk'][:200]}...\")\n",
    "    \n",
    "    # Evaluate the result\n",
    "    print(f\"\\nüìä EVALUATION RESULTS:\")\n",
    "    evaluation_scores = enhanced_rag.evaluate_query_result(result)\n",
    "    \n",
    "    if evaluation_scores:\n",
    "        for metric, score in evaluation_scores.items():\n",
    "            if not metric.endswith('_reason'):\n",
    "                reason_key = f\"{metric}_reason\"\n",
    "                reason = evaluation_scores.get(reason_key, \"N/A\")\n",
    "                print(f\"  ‚Ä¢ {metric.upper()}: {score}/5\")\n",
    "                if reason != \"N/A\":\n",
    "                    print(f\"    Reasoning: {reason[:100]}...\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No evaluation scores available (Azure OpenAI not configured)\")\n",
    "    \n",
    "    return result, evaluation_scores\n",
    "\n",
    "# Test with sample questions\n",
    "print(\"üöÄ Testing Enhanced RAG System with Azure AI Foundry Evaluators\")\n",
    "\n",
    "# Test 1: Key objectives\n",
    "result1, scores1 = test_query_with_evaluation(sample_questions[0])\n",
    "\n",
    "# Test 2: Ethical considerations  \n",
    "result2, scores2 = test_query_with_evaluation(sample_questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3f780a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ RUNNING BATCH EVALUATION ON 4 QUESTIONS\n",
      "============================================================\n",
      "\n",
      "[1/4] Processing: What are the key objectives of the National AI Str...\n",
      "  ‚úÖ Avg evaluation score: 3.75/5\n",
      "\n",
      "[2/4] Processing: How does the strategy address ethical consideratio...\n",
      "  ‚úÖ Avg evaluation score: 3.00/5\n",
      "\n",
      "[3/4] Processing: What are the main challenges identified in the str...\n",
      "  ‚úÖ Avg evaluation score: 1.50/5\n",
      "\n",
      "[4/4] Processing: What is the role of public-private partnerships in...\n",
      "  ‚úÖ Avg evaluation score: 2.00/5\n",
      "\n",
      "üìà BATCH EVALUATION SUMMARY\n",
      "========================================\n",
      "  ‚Ä¢ Average COHERENCE: 2.50/5\n",
      "  ‚Ä¢ Average GROUNDEDNESS: 2.50/5\n",
      "  ‚Ä¢ Average FLUENCY: 3.00/5\n",
      "  ‚Ä¢ Average RELEVANCE: 2.25/5\n",
      "\n",
      "üîç OBSERVABILITY SUMMARY\n",
      "==============================\n",
      "  ‚Ä¢ Total Queries: 6\n",
      "  ‚Ä¢ Average Retrieval Score: 0.5321155687173208\n",
      "  ‚Ä¢ Latest Query: What is the role of public-private partnerships in the strategy?\n",
      "  ‚Ä¢ Chunks Per Query: 5\n",
      "‚úÖ Saved 6 entries to rag_evaluation_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Batch evaluation and observability dashboard\n",
    "def run_batch_evaluation(questions: List[str], save_results: bool = True):\n",
    "    \"\"\"Run batch evaluation on multiple questions\"\"\"\n",
    "    print(f\"\\nüîÑ RUNNING BATCH EVALUATION ON {len(questions)} QUESTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    batch_results = []\n",
    "    batch_scores = []\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n[{i}/{len(questions)}] Processing: {question[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Get result\n",
    "            result = enhanced_rag.query_with_context(question)\n",
    "            \n",
    "            # Evaluate\n",
    "            scores = enhanced_rag.evaluate_query_result(result)\n",
    "            \n",
    "            batch_results.append(result)\n",
    "            batch_scores.append(scores)\n",
    "            \n",
    "            # Show brief progress\n",
    "            if scores:\n",
    "                avg_score = np.mean([v for k, v in scores.items() if not k.endswith('_reason')])\n",
    "                print(f\"  ‚úÖ Avg evaluation score: {avg_score:.2f}/5\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è No evaluation scores (Azure OpenAI not configured)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìà BATCH EVALUATION SUMMARY\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if batch_scores and any(batch_scores):\n",
    "        # Calculate average scores for each metric\n",
    "        metrics = set()\n",
    "        for score_dict in batch_scores:\n",
    "            metrics.update([k for k in score_dict.keys() if not k.endswith('_reason')])\n",
    "        \n",
    "        for metric in metrics:\n",
    "            scores_for_metric = [s.get(metric, 0) for s in batch_scores if s.get(metric, 0) > 0]\n",
    "            if scores_for_metric:\n",
    "                avg_score = np.mean(scores_for_metric)\n",
    "                print(f\"  ‚Ä¢ Average {metric.upper()}: {avg_score:.2f}/5\")\n",
    "    \n",
    "    # Observability summary\n",
    "    obs_summary = enhanced_rag.get_observability_summary()\n",
    "    print(f\"\\nüîç OBSERVABILITY SUMMARY\")\n",
    "    print(\"=\"*30)\n",
    "    for key, value in obs_summary.items():\n",
    "        print(f\"  ‚Ä¢ {key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    # Save evaluation dataset\n",
    "    if save_results:\n",
    "        enhanced_rag.save_evaluation_dataset()\n",
    "    \n",
    "    return batch_results, batch_scores\n",
    "\n",
    "# Run batch evaluation on all sample questions\n",
    "batch_results, batch_scores = run_batch_evaluation(sample_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31237805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-projects in ./.venv/lib/python3.13/site-packages (1.0.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in ./.venv/lib/python3.13/site-packages (from azure-ai-projects) (0.7.2)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in ./.venv/lib/python3.13/site-packages (from azure-ai-projects) (1.35.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.13/site-packages (from azure-ai-projects) (4.14.1)\n",
      "Requirement already satisfied: azure-storage-blob>=12.15.0 in ./.venv/lib/python3.13/site-packages (from azure-ai-projects) (12.26.0)\n",
      "Requirement already satisfied: azure-ai-agents>=1.0.0 in ./.venv/lib/python3.13/site-packages (from azure-ai-projects) (1.0.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in ./.venv/lib/python3.13/site-packages (from azure-core>=1.30.0->azure-ai-projects) (2.32.4)\n",
      "Requirement already satisfied: six>=1.11.0 in ./.venv/lib/python3.13/site-packages (from azure-core>=1.30.0->azure-ai-projects) (1.17.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in ./.venv/lib/python3.13/site-packages (from azure-storage-blob>=12.15.0->azure-ai-projects) (45.0.5)\n",
      "Requirement already satisfied: cffi>=1.14 in ./.venv/lib/python3.13/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.15.0->azure-ai-projects) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.13/site-packages (from cffi>=1.14->cryptography>=2.1.4->azure-storage-blob>=12.15.0->azure-ai-projects) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-projects) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-projects) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-projects) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-projects) (2025.7.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-ai-projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1924912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure AI Foundry project client initialized\n",
      "‚úÖ Azure AI Foundry integration and observability setup complete!\n",
      "\n",
      " Available Features:\n",
      "  ‚Ä¢ Enhanced RAG with evaluation support\n",
      "  ‚Ä¢ Azure AI Foundry evaluators (Groundedness, Relevance, Coherence, Fluency)\n",
      "  ‚Ä¢ Batch evaluation capabilities\n",
      "  ‚Ä¢ Observability and monitoring\n",
      "  ‚Ä¢ Cloud evaluation integration (when configured)\n",
      "  ‚Ä¢ Evaluation dataset export (JSONL format)\n",
      "\n",
      " Next Steps:\n",
      "  1. Set Azure OpenAI environment variables for AI-assisted evaluation\n",
      "  2. Set AZURE_AI_PROJECT_ENDPOINT for cloud evaluation\n",
      "  3. Run queries and monitor performance metrics\n",
      "  4. Export evaluation datasets for further analysis\n"
     ]
    }
   ],
   "source": [
    "# Azure AI Foundry Cloud Evaluation Integration\n",
    "def setup_cloud_evaluation():\n",
    "    \"\"\"\n",
    "    Set up cloud evaluation with Azure AI Foundry\n",
    "    Requires Azure AI Foundry project\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from azure.ai.projects import AIProjectClient\n",
    "        from azure.identity import DefaultAzureCredential\n",
    "        \n",
    "        # These should be set in your environment variables\n",
    "        project_endpoint = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")  # e.g., \"https://<project-name>.services.ai.azure.com/api/projects/<project-id>\"\n",
    "        \n",
    "        if not project_endpoint:\n",
    "            print(\"‚ö†Ô∏è Azure AI Foundry project endpoint not configured\")\n",
    "            print(\"Set AZURE_AI_PROJECT_ENDPOINT environment variable to use cloud evaluation\")\n",
    "            return None\n",
    "        \n",
    "        # Create project client\n",
    "        credential = DefaultAzureCredential()\n",
    "        project_client = AIProjectClient(\n",
    "            endpoint=project_endpoint,\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Azure AI Foundry project client initialized\")\n",
    "        return project_client\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Azure AI Projects SDK not installed\")\n",
    "        print(\"Run: pip install azure-ai-projects\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error setting up cloud evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_cloud_evaluation(project_client, dataset_path: str = \"rag_evaluation_dataset.jsonl\"):\n",
    "    \"\"\"\n",
    "    Run evaluation in Azure AI Foundry cloud\n",
    "    \"\"\"\n",
    "    if not project_client:\n",
    "        print(\"‚ùå No project client available for cloud evaluation\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # This is a placeholder for cloud evaluation\n",
    "        # The actual implementation would depend on your Azure AI Foundry project setup\n",
    "        print(f\"üå©Ô∏è Running cloud evaluation with dataset: {dataset_path}\")\n",
    "        print(\"üìä Results would be available in Azure AI Foundry portal\")\n",
    "        print(\"üîó Check your Azure AI Foundry project for detailed evaluation results\")\n",
    "        \n",
    "        # In a real implementation, you would:\n",
    "        # 1. Upload the dataset to your Azure AI Foundry project\n",
    "        # 2. Run the evaluation using the project client\n",
    "        # 3. Monitor the results in the Azure AI Foundry portal\n",
    "        \n",
    "        return \"cloud_evaluation_placeholder\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running cloud evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Set up cloud evaluation (optional)\n",
    "project_client = setup_cloud_evaluation()\n",
    "\n",
    "# Create monitoring and logging utilities\n",
    "class RAGObservability:\n",
    "    \"\"\"Enhanced observability for RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system):\n",
    "        self.rag_system = rag_system\n",
    "        self.evaluation_history = []\n",
    "    \n",
    "    def log_evaluation(self, query: str, result: dict, scores: dict):\n",
    "        \"\"\"Log evaluation for monitoring\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': str(np.datetime64('now')),\n",
    "            'query': query,\n",
    "            'response_length': len(result.get('response', '')),\n",
    "            'context_length': len(result.get('context', '')),\n",
    "            'num_retrieved_chunks': len(result.get('retrieved_chunks', [])),\n",
    "            'scores': scores,\n",
    "            'retrieval_score': result.get('metadata', {}).get('top_score', 0)\n",
    "        }\n",
    "        self.evaluation_history.append(log_entry)\n",
    "    \n",
    "    def get_performance_metrics(self):\n",
    "        \"\"\"Get performance metrics over time\"\"\"\n",
    "        if not self.evaluation_history:\n",
    "            return {}\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Average evaluation scores\n",
    "        all_scores = {}\n",
    "        for entry in self.evaluation_history:\n",
    "            for metric, score in entry['scores'].items():\n",
    "                if not metric.endswith('_reason') and isinstance(score, (int, float)):\n",
    "                    if metric not in all_scores:\n",
    "                        all_scores[metric] = []\n",
    "                    all_scores[metric].append(score)\n",
    "        \n",
    "        for metric, scores in all_scores.items():\n",
    "            metrics[f'avg_{metric}'] = np.mean(scores)\n",
    "            metrics[f'std_{metric}'] = np.std(scores)\n",
    "        \n",
    "        # System performance metrics\n",
    "        metrics['avg_response_length'] = np.mean([e['response_length'] for e in self.evaluation_history])\n",
    "        metrics['avg_context_length'] = np.mean([e['context_length'] for e in self.evaluation_history])\n",
    "        metrics['avg_retrieval_score'] = np.mean([e['retrieval_score'] for e in self.evaluation_history])\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def export_monitoring_data(self, filename: str = \"rag_monitoring.json\"):\n",
    "        \"\"\"Export monitoring data for external analysis\"\"\"\n",
    "        monitoring_data = {\n",
    "            'system_info': {\n",
    "                'total_chunks': len(self.rag_system.chunks),\n",
    "                'embedding_model': 'all-MiniLM-L6-v2',\n",
    "                'total_evaluations': len(self.evaluation_history)\n",
    "            },\n",
    "            'performance_metrics': self.get_performance_metrics(),\n",
    "            'evaluation_history': self.evaluation_history\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(monitoring_data, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Monitoring data exported to {filename}\")\n",
    "\n",
    "# Initialize observability\n",
    "rag_observability = RAGObservability(enhanced_rag)\n",
    "\n",
    "print(\"‚úÖ Azure AI Foundry integration and observability setup complete!\")\n",
    "print(\"\\n Available Features:\")\n",
    "print(\"  ‚Ä¢ Enhanced RAG with evaluation support\")\n",
    "print(\"  ‚Ä¢ Azure AI Foundry evaluators (Groundedness, Relevance, Coherence, Fluency)\")\n",
    "print(\"  ‚Ä¢ Batch evaluation capabilities\")\n",
    "print(\"  ‚Ä¢ Observability and monitoring\")\n",
    "print(\"  ‚Ä¢ Cloud evaluation integration (when configured)\")\n",
    "print(\"  ‚Ä¢ Evaluation dataset export (JSONL format)\")\n",
    "print(\"\\n Next Steps:\")\n",
    "print(\"  1. Set Azure OpenAI environment variables for AI-assisted evaluation\")\n",
    "print(\"  2. Set AZURE_AI_PROJECT_ENDPOINT for cloud evaluation\")\n",
    "print(\"  3. Run queries and monitor performance metrics\")\n",
    "print(\"  4. Export evaluation datasets for further analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
